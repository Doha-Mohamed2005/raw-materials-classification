{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC7KETDhdQC6"
      },
      "source": [
        "## Mount Google Drive and Link Dataset\n",
        "\n",
        "In this step, we mount Google Drive to access the dataset stored in the project directory.  \n",
        "We also create symbolic links to the images folder and the annotations file to make them accessible directly from the Colab environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc_vCJSOfUH-",
        "outputId": "bef108d7-1db7-41d4-b2f7-fa03cee58da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Kaggle API setup completed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "unzip:  cannot find or open material-dataset-new.zip, material-dataset-new.zip.zip or material-dataset-new.zip.ZIP.\n",
            "\n",
            "=== Creating local symlinks ===\n",
            "ln: failed to create symbolic link '/content/images': File exists\n",
            "ln: failed to create symbolic link '/content/annotations.json': File exists\n",
            "\n",
            "âœ… Data is ready via Kaggle API!\n"
          ]
        }
      ],
      "source": [
        "# CELL 1 â€“ Setup Kaggle API, Download Data, & Create Symlinks\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# 1. Install and set up Kaggle API (assuming kaggle.json is uploaded)\n",
        "!pip install -q kaggle\n",
        "!apt-get -qq install -y unzip\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"Kaggle API setup completed.\")\n",
        "\n",
        "# 2. Download and unzip the dataset\n",
        "DATASET_NAME = 'haaroonafroz/material-dataset-new'\n",
        "DOWNLOAD_ZIP = 'material-dataset-new.zip'\n",
        "DESTINATION_FOLDER = 'dataset_raw'\n",
        "\n",
        "!kaggle datasets download -d {DATASET_NAME}\n",
        "!unzip -q {DOWNLOAD_ZIP} -d {DESTINATION_FOLDER}\n",
        "\n",
        "# 3. Define the new local source paths\n",
        "SOURCE_BASE = os.path.join(DESTINATION_FOLDER, 'Material_dataset')\n",
        "IMAGES_PATH = os.path.join(SOURCE_BASE, 'JPEGImages')\n",
        "ANNOTATIONS_PATH = os.path.join(SOURCE_BASE, 'annotations.json')\n",
        "\n",
        "# 4. Create local symlinks to maintain compatibility with later cells\n",
        "# The old code relied on these symlinks: /content/images and /content/annotations.json\n",
        "print(\"\\n=== Creating local symlinks ===\")\n",
        "!ln -s \"{IMAGES_PATH}\" /content/images\n",
        "!ln -s \"{ANNOTATIONS_PATH}\" /content/annotations.json\n",
        "\n",
        "print(\"\\nâœ… Data is ready via Kaggle API!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXZBMjV7dYqg"
      },
      "source": [
        "## Load and Prepare Dataset from Google Drive\n",
        "\n",
        "In this step, we connect Google Colab to Google Drive in order to access the dataset used for training.  \n",
        "We then create symbolic links to the images folder and the annotations file so that they can be easily accessed within the Colab environment.  \n",
        "Finally, we verify that the dataset has been linked correctly by printing the number of images and showing a few sample filenames.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ygga5wUrxvZ",
        "outputId": "3af99a63-90b4-4429-a519-d700a0a93b37"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/annotations.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-649618969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/annotations.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/annotations.json'"
          ]
        }
      ],
      "source": [
        "# CELL 2 â€“ Load + Fix Filenames & Class Labels (Critical Preprocessing)\n",
        "\n",
        "import json, os\n",
        "\n",
        "\n",
        "\n",
        "# Load annotations\n",
        "\n",
        "with open('/content/annotations.json', 'r') as f:\n",
        "\n",
        "    data = json.load(f)['annotations']\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Total annotations: {len(data)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Get real image filenames\n",
        "\n",
        "real_images = set(os.listdir('/content/images'))\n",
        "\n",
        "\n",
        "\n",
        "# Fix filenames\n",
        "\n",
        "fixed = 0\n",
        "\n",
        "for item in data:\n",
        "\n",
        "    old = item['image']\n",
        "\n",
        "    if old.startswith(\"image_\"):\n",
        "\n",
        "        num = old[6:10]\n",
        "\n",
        "        mat = old[11] if len(old) >= 12 else 'm'\n",
        "\n",
        "        new1 = f\"rendered_image_{num}_{mat}.jpg\"\n",
        "\n",
        "        new2 = f\"rendered_image_{num}_mx.jpg\"\n",
        "\n",
        "        if new1 in real_images:\n",
        "\n",
        "            item['image'] = new1\n",
        "\n",
        "            fixed += 1\n",
        "\n",
        "        elif new2 in real_images:\n",
        "\n",
        "            item['image'] = new2\n",
        "\n",
        "            fixed += 1\n",
        "\n",
        "\n",
        "\n",
        "# Normalize class labels\n",
        "\n",
        "for item in data:\n",
        "\n",
        "    label = item['class_label'].lower()\n",
        "\n",
        "    if label in ['mix', 'mx', 'mixed']:\n",
        "\n",
        "        item['class_label'] = 'mixed'\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Fixed {fixed}/{len(data)} image names\")\n",
        "\n",
        "\n",
        "\n",
        "# Define class mapping\n",
        "\n",
        "classes = ['glass', 'metal', 'mixed', 'plastic', 'wood']\n",
        "\n",
        "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
        "\n",
        "idx_to_class = {i: c for i, c in enumerate(classes)}\n",
        "\n",
        "\n",
        "\n",
        "print(\"Classes:\", classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLB_RowmdkHh"
      },
      "source": [
        "## Visualizing Sample Images from the Dataset\n",
        "\n",
        "In this step, we randomly select a few samples from the dataset and display them using Matplotlib.  \n",
        "This helps verify that the images are loaded correctly and that the class labels match the corresponding files before training the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "vGFTGN2WyG7x",
        "outputId": "31f3b33f-d83a-457a-f790-15a0b1aee433"
      },
      "outputs": [],
      "source": [
        "# CELL 3 â€“ Show Sample Data\n",
        "\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "# Random sample\n",
        "\n",
        "sample = random.sample(data, 5)\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "\n",
        "\n",
        "\n",
        "for ax, item in zip(axes, sample):\n",
        "\n",
        "    img_path = os.path.join('/content/images', item['image'])\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    ax.imshow(img)\n",
        "\n",
        "    ax.set_title(f\"{item['class_label']}\\n{item['image']}\")\n",
        "\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEu0djQLdrHO"
      },
      "source": [
        "## Data Cleaning & Verification (Post-Preprocessing)\n",
        "\n",
        "In this step, we analyze and clean the dataset after preprocessing to ensure data consistency and integrity.\n",
        "\n",
        "### This cell performs the following tasks:\n",
        "\n",
        "1. Convert the annotations into a Pandas DataFrame for easier analysis.\n",
        "2. Check the class distribution to understand dataset balance.\n",
        "3. Identify:\n",
        "   - Images that exist without annotations.\n",
        "   - Annotations that reference missing images.\n",
        "4. Remove invalid entries where annotation does not have a corresponding image.\n",
        "5. Recalculate and visualize the cleaned class distribution.\n",
        "6. Update the `data` variable with the cleaned dataset.\n",
        "\n",
        "A bar chart is also displayed to show the final class distribution after data cleaning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SIGJdUxMyf24",
        "outputId": "54c0a79c-d694-4812-9220-a3bcd0a79f64"
      },
      "outputs": [],
      "source": [
        "# CELL 4 â€“ Data Cleaning & Verification (Post-Preprocessing)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"DATA CLEANING & VERIFICATION\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "\n",
        "# Convert annotations to DataFrame for easier analysis\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n",
        "# 1. Check class distribution\n",
        "\n",
        "print(\"\\n1. CLASS DISTRIBUTION:\")\n",
        "\n",
        "print(df['class_label'].value_counts())\n",
        "\n",
        "print(\"-\"*40)\n",
        "\n",
        "\n",
        "\n",
        "# 2. Find mismatches between images and annotations\n",
        "\n",
        "fixed_images = set(df['image'])\n",
        "\n",
        "real_images = set(os.listdir('/content/images'))\n",
        "\n",
        "\n",
        "\n",
        "# Images without annotations\n",
        "\n",
        "images_without_annots = real_images - fixed_images\n",
        "\n",
        "print(f\"\\n2. IMAGES WITHOUT ANNOTATIONS: {len(images_without_annots)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Annotations without corresponding images\n",
        "\n",
        "annots_without_images = [img for img in df['image'] if img not in real_images]\n",
        "\n",
        "print(f\"ANNOTATIONS WITHOUT IMAGES: {len(annots_without_images)}\")\n",
        "\n",
        "\n",
        "\n",
        "if annots_without_images:\n",
        "\n",
        "    print(f\"Sample missing images: {annots_without_images[:5]}\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. Remove annotations without images\n",
        "\n",
        "print(f\"\\n3. CLEANING DATA...\")\n",
        "\n",
        "print(f\"Total annotations before cleaning: {len(df)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Remove rows where image doesn't exist\n",
        "\n",
        "df_clean = df[df['image'].isin(real_images)].copy()\n",
        "\n",
        "print(f\"Total annotations after cleaning: {len(df_clean)}\")\n",
        "\n",
        "\n",
        "\n",
        "# 4. Check cleaned distribution\n",
        "\n",
        "print(f\"\\n4. CLEANED CLASS DISTRIBUTION:\")\n",
        "\n",
        "class_dist = df_clean['class_label'].value_counts()\n",
        "\n",
        "print(class_dist)\n",
        "\n",
        "\n",
        "\n",
        "# Visualize distribution\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "class_dist.plot(kind='bar', color='skyblue')\n",
        "\n",
        "plt.title('Class Distribution After Cleaning')\n",
        "\n",
        "plt.xlabel('Class')\n",
        "\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# 5. Update data with cleaned version\n",
        "\n",
        "data = df_clean.to_dict('records')\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\n5. FINAL DATASET INFO:\")\n",
        "\n",
        "print(f\"Total images: {len(real_images)}\")\n",
        "\n",
        "print(f\"Total annotations: {len(data)}\")\n",
        "\n",
        "print(f\"Classes: {list(class_dist.index)}\")\n",
        "\n",
        "print(f\"Class counts: {class_dist.to_dict()}\")\n",
        "\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD8mlxGSd1M_"
      },
      "source": [
        "## Trainâ€“Validationâ€“Test Split\n",
        "\n",
        "After cleaning and verifying the dataset, this step divides the images into three separate subsets:\n",
        "\n",
        "### 1. Training Set\n",
        "Used to train the CNN model.\n",
        "\n",
        "### 2. Validation Set\n",
        "Used to tune hyperparameters and monitor performance during training to prevent overfitting.\n",
        "\n",
        "### 3. Test Set\n",
        "Used to evaluate the final model after training is complete.\n",
        "\n",
        "### This cell performs the following:\n",
        "- Builds lists of `image_paths` and numerical `labels`.\n",
        "- Applies stratified splitting using `train_test_split` to preserve class balance.\n",
        "- Saves the split into a `dataset_split.pkl` file for reproducibility.\n",
        "- Prints detailed statistics including:\n",
        "  - Percentages of train/validation/test sets\n",
        "  - Class distribution per split\n",
        "\n",
        "This ensures a well-balanced and reproducible dataset split for training deep learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFi2VtjUy9tj",
        "outputId": "b9f5ab3e-5575-45b9-8e9d-d9ef69ed4faf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "image_paths = []\n",
        "\n",
        "labels = []\n",
        "\n",
        "\n",
        "\n",
        "for item in data:\n",
        "\n",
        "    img_path = os.path.join('/content/images', item['image'])\n",
        "\n",
        "    if os.path.exists(img_path):\n",
        "\n",
        "        image_paths.append(img_path)\n",
        "\n",
        "        labels.append(class_to_idx[item['class_label'].lower()])\n",
        "\n",
        "\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "\n",
        "    image_paths, labels,\n",
        "\n",
        "    test_size=0.15,\n",
        "\n",
        "    stratify=labels,\n",
        "\n",
        "    random_state=42\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "\n",
        "    X_temp, y_temp,\n",
        "\n",
        "    test_size=0.1765,\n",
        "\n",
        "    stratify=y_temp,\n",
        "\n",
        "    random_state=42\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "split_data = {\n",
        "\n",
        "    'X_train': X_train, 'y_train': y_train,\n",
        "\n",
        "    'X_val': X_val, 'y_val': y_val,\n",
        "\n",
        "    'X_test': X_test, 'y_test': y_test,\n",
        "\n",
        "    'classes': classes,\n",
        "\n",
        "    'class_to_idx': class_to_idx,\n",
        "\n",
        "    'idx_to_class': idx_to_class\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù\n",
        "\n",
        "with open('/content/dataset_split.pkl', 'wb') as f:\n",
        "\n",
        "    pickle.dump(split_data, f)\n",
        "\n",
        "\n",
        "\n",
        "print(\"âœ… Split saved to '/content/dataset_split.pkl'\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_split():\n",
        "\n",
        "    with open('/content/dataset_split.pkl', 'rb') as f:\n",
        "\n",
        "        split_data = pickle.load(f)\n",
        "\n",
        "    return split_data\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nðŸ“Œ To load this split later, use:\")\n",
        "\n",
        "print(\"split_data = load_split()\")\n",
        "\n",
        "print(\"X_train, y_train = split_data['X_train'], split_data['y_train']\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nâœ… Split completed!\")\n",
        "\n",
        "print(f\"Total images: {len(image_paths)}\")\n",
        "\n",
        "print(f\"Train: {len(X_train)} ({len(X_train)/len(image_paths)*100:.1f}%)\")\n",
        "\n",
        "print(f\"Validation: {len(X_val)} ({len(X_val)/len(image_paths)*100:.1f}%)\")\n",
        "\n",
        "print(f\"Test: {len(X_test)} ({len(X_test)/len(image_paths)*100:.1f}%)\")\n",
        "\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "\n",
        "print(f\"Train: {Counter(y_train)}\")\n",
        "\n",
        "print(f\"Val: {Counter(y_val)}\")\n",
        "\n",
        "print(f\"Test: {Counter(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBC7LJGsd9rD"
      },
      "source": [
        "## Organizing Dataset Folders\n",
        "\n",
        "This step organizes the dataset into structured folders by class for training and validation.  \n",
        "Each split (`train` and `val`) will have subfolders for each class (`glass`, `metal`, `mixed`, `plastic`, `wood`).  \n",
        "\n",
        "Images are copied to their corresponding class folders, and a progress bar shows the copy status.  \n",
        "At the end, the number of images per split and class distribution are printed to verify the dataset organization.\n",
        "\n",
        "This structure is required for most deep learning data loaders and ensures a clean, reproducible workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2ArH12pzHHW",
        "outputId": "37f4f8c0-e3b9-4b74-e494-8e974b4ed000"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import shutil\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"CREATING ORGANIZED FOLDERS\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "if 'idx_to_class' not in locals():\n",
        "\n",
        "    classes = ['glass', 'metal', 'mixed', 'plastic', 'wood']\n",
        "\n",
        "    idx_to_class = {i: c for i, c in enumerate(classes)}\n",
        "\n",
        "\n",
        "\n",
        "def create_folders(X_split, y_split, split_name):\n",
        "\n",
        "    base_path = f\"/content/data/{split_name}\"\n",
        "\n",
        "\n",
        "\n",
        "    for class_name in classes:\n",
        "\n",
        "        os.makedirs(os.path.join(base_path, class_name), exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    copied = 0\n",
        "\n",
        "    for img_path, label_idx in tqdm(zip(X_split, y_split),\n",
        "\n",
        "                                    total=len(X_split),\n",
        "\n",
        "                                    desc=f\"Copying {split_name}\"):\n",
        "\n",
        "        try:\n",
        "\n",
        "            class_name = idx_to_class[label_idx]\n",
        "\n",
        "            filename = os.path.basename(img_path)\n",
        "\n",
        "            dest_path = os.path.join(base_path, class_name, filename)\n",
        "\n",
        "            shutil.copy2(img_path, dest_path)\n",
        "\n",
        "            copied += 1\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            print(f\"Error: {img_path} â†’ {e}\")\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"âœ… {split_name}: {copied}/{len(X_split)} images\")\n",
        "\n",
        "    print(f\"\\n{split_name} class distribution:\")\n",
        "\n",
        "    for class_name in classes:\n",
        "\n",
        "        count = len(os.listdir(os.path.join(base_path, class_name)))\n",
        "\n",
        "        print(f\"  {class_name}: {count}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "    return base_path\n",
        "\n",
        "os.makedirs(\"/content/data\", exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "train_base = create_folders(X_train, y_train, \"train\")\n",
        "\n",
        "val_base = create_folders(X_val, y_val, \"val\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"FOLDERS CREATED SUCCESSFULLY!\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"ðŸ“ Train: {train_base}\")\n",
        "\n",
        "print(f\"ðŸ“ Val: {val_base}\")\n",
        "\n",
        "print(f\"ðŸ“ Test: Will be used from variables (X_test, y_test)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW_c5QKieGq2"
      },
      "source": [
        "## Data Augmentation and DataLoaders\n",
        "\n",
        "In this step, we prepare the dataset for training by applying transformations and creating PyTorch DataLoaders.\n",
        "\n",
        "### Transformations:\n",
        "\n",
        "- **Training set**: Random resized crop, horizontal flip, rotation, color jitter, normalization.  \n",
        "- **Validation set**: Resize and normalization only (no augmentation).\n",
        "\n",
        "### DataLoaders:\n",
        "\n",
        "- `train_loader` and `val_loader` are created with a batch size of 32.  \n",
        "- Shuffling is enabled for training to improve generalization.  \n",
        "- These DataLoaders will feed batches of images to the CNN during training and validation.\n",
        "\n",
        "This step ensures that the model sees augmented data during training while evaluation is done on consistent, normalized images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJT7KDmmzQID",
        "outputId": "6daf21dd-a397-4139-99bc-0934374cf5a6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "\n",
        "# Transformations\n",
        "\n",
        "train_transform_advanced = transforms.Compose([\n",
        "\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "\n",
        "    transforms.Resize((224, 224)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "\n",
        "    root='/content/data/train',\n",
        "\n",
        "    transform=train_transform_advanced\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "val_dataset = datasets.ImageFolder(\n",
        "\n",
        "    root='/content/data/val',\n",
        "\n",
        "    transform=val_test_transform\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "classes = train_dataset.classes\n",
        "\n",
        "class_to_idx = train_dataset.class_to_idx\n",
        "\n",
        "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "\n",
        "\n",
        "\n",
        "print(f\"âœ… Classes detected: {classes}\")\n",
        "\n",
        "print(f\"âœ… Class to index: {class_to_idx}\")\n",
        "\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nâœ… DataLoaders created:\")\n",
        "\n",
        "print(f\"  Train: {len(train_dataset)} samples\")\n",
        "\n",
        "print(f\"  Validation: {len(val_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq6Azbh-eN52"
      },
      "source": [
        "## Creating PyTorch DataLoaders\n",
        "\n",
        "In this step, we define a custom PyTorch `Dataset` class (`MaterialDataset`) to handle our images and labels.  \n",
        "We also create DataLoaders for training, validation, and testing.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "- **Custom Dataset**:\n",
        "  - Loads images from paths and applies optional transformations.\n",
        "  - Returns a tuple `(image, label)` for each sample.\n",
        "\n",
        "- **Transformations**:\n",
        "  - **Training**: Random resized crop, horizontal flip, rotation, color jitter, normalization.\n",
        "  - **Validation/Test**: Resize to 224Ã—224 and normalization only.\n",
        "\n",
        "- **DataLoaders**:\n",
        "  - `train_loader` with shuffling for better generalization.\n",
        "  - `val_loader` and `test_loader` without shuffling.\n",
        "  - Batch size = 32.\n",
        "\n",
        "These DataLoaders provide batches of images and labels to feed the CNN during training and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "carxszlDzVfn",
        "outputId": "a1221b6c-a1a8-4def-b14e-06a87a68a256"
      },
      "outputs": [],
      "source": [
        "# CELL 7 â€“ Create PyTorch DataLoader\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "class MaterialDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "\n",
        "        self.labels = labels\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = self.image_paths[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "\n",
        "            image = self.transform(image)\n",
        "\n",
        "\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "\n",
        "# Transformations\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "\n",
        "    transforms.RandomResizedCrop(224),\n",
        "\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "\n",
        "    transforms.RandomRotation(10),\n",
        "\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "\n",
        "    transforms.Resize((224, 224)),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "\n",
        "train_dataset = MaterialDataset(X_train, y_train, transform=train_transform)\n",
        "\n",
        "val_dataset = MaterialDataset(X_val, y_val, transform=val_test_transform)\n",
        "\n",
        "test_dataset = MaterialDataset(X_test, y_test, transform=val_test_transform)\n",
        "\n",
        "\n",
        "\n",
        "# DataLoaders\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "print(\"DataLoaders created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF82u2_weS-A"
      },
      "source": [
        "## Advanced Data Augmentation with torchvision\n",
        "\n",
        "In this step, we enhance the training dataset using advanced augmentation techniques to improve model robustness, especially for texture and lighting variations.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "- **Advanced Augmentations for Training:**\n",
        "  - Random resized crop, horizontal flip, rotation\n",
        "  - Color jitter (brightness, contrast, saturation, hue)\n",
        "  - Sharpness adjustment, autocontrast, grayscale\n",
        "  - Gaussian blur\n",
        "  - Simulated Gaussian noise\n",
        "\n",
        "- **Validation/Test:**  \n",
        "  Only resize and normalization, no augmentation.\n",
        "\n",
        "- **Custom Dataset Class (`MaterialDataset`):**  \n",
        "  - Loads images and applies the chosen transformations\n",
        "  - Handles errors gracefully by returning a dummy image if loading fails\n",
        "\n",
        "- **DataLoaders:**  \n",
        "  - `train_aug_loader` with shuffling for training  \n",
        "  - `val_aug_loader` and `test_aug_loader` without shuffling  \n",
        "  - Batch size = 32\n",
        "\n",
        "These augmentations help the CNN model generalize better under varying lighting, texture, and industrial noise conditions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "580xluqRzhjH",
        "outputId": "dcd0aa74-873f-4885-b6ee-b75b2eb701e0"
      },
      "outputs": [],
      "source": [
        "# CELL 8 â€“ Advanced Augmentation using torchvision (WORKING VERSION)\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "print(\"Setting up advanced augmentation pipeline using torchvision...\")\n",
        "\n",
        "\n",
        "\n",
        "def get_advanced_torchvision_augmentations():\n",
        "\n",
        "\n",
        "\n",
        "    return transforms.Compose([\n",
        "\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "\n",
        "\n",
        "\n",
        "        # Lighting variations (BONUS FOCUS)\n",
        "\n",
        "        transforms.ColorJitter(\n",
        "\n",
        "            brightness=0.3,    # Â±30% brightness variation\n",
        "\n",
        "            contrast=0.3,      # Â±30% contrast variation\n",
        "\n",
        "            saturation=0.2,    # Â±20% saturation\n",
        "\n",
        "            hue=0.1           # Â±10% hue\n",
        "\n",
        "        ),\n",
        "\n",
        "\n",
        "\n",
        "        # Advanced augmentations for texture robustness\n",
        "\n",
        "        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
        "\n",
        "        transforms.RandomAutocontrast(p=0.3),\n",
        "\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "\n",
        "\n",
        "\n",
        "        # Simulate shadows and blur\n",
        "\n",
        "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "\n",
        "\n",
        "\n",
        "        # Convert to tensor\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "\n",
        "\n",
        "        # Normalize (ImageNet stats)\n",
        "\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "\n",
        "                           std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "\n",
        "\n",
        "        # Add simulated noise for industrial conditions\n",
        "\n",
        "        transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.05),  # Gaussian noise\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "def get_basic_transform():\n",
        "\n",
        "\n",
        "\n",
        "    return transforms.Compose([\n",
        "\n",
        "        transforms.Resize((224, 224)),\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "\n",
        "                           std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "# Dataset Class\n",
        "\n",
        "class MaterialDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "\n",
        "        self.labels = labels\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_path = self.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            label = self.labels[idx]\n",
        "\n",
        "\n",
        "\n",
        "            if self.transform:\n",
        "\n",
        "                image = self.transform(image)\n",
        "\n",
        "\n",
        "\n",
        "            return image, label\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "            # Return a dummy image if there's an error\n",
        "\n",
        "            dummy_img = torch.zeros((3, 224, 224))\n",
        "\n",
        "            return dummy_img, 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_transform_advanced = get_advanced_torchvision_augmentations()\n",
        "\n",
        "val_transform = get_basic_transform()\n",
        "\n",
        "test_transform = get_basic_transform()\n",
        "\n",
        "print(f\"\\nData info:\")\n",
        "\n",
        "print(f\"X_train: {len(X_train)} samples\")\n",
        "\n",
        "print(f\"X_val: {len(X_val)} samples\")\n",
        "\n",
        "print(f\"X_test: {len(X_test)} samples\")\n",
        "\n",
        "train_aug_dataset = MaterialDataset(X_train, y_train, transform=train_transform_advanced)\n",
        "\n",
        "val_aug_dataset = MaterialDataset(X_val, y_val, transform=val_transform)\n",
        "\n",
        "test_aug_dataset = MaterialDataset(X_test, y_test, transform=test_transform)\n",
        "\n",
        "train_aug_loader = DataLoader(train_aug_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "val_aug_loader = DataLoader(val_aug_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "test_aug_loader = DataLoader(test_aug_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nâœ“ Datasets created successfully!\")\n",
        "\n",
        "print(f\"  Train dataset: {len(train_aug_dataset)} samples\")\n",
        "\n",
        "print(f\"  Validation dataset: {len(val_aug_dataset)} samples\")\n",
        "\n",
        "print(f\"  Test dataset: {len(test_aug_dataset)} samples\")\n",
        "\n",
        "print(f\"  Classes: {classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fMfqx8yegFV"
      },
      "source": [
        "## Visualizing Augmented Samples\n",
        "\n",
        "In this step, we visualize the effect of advanced augmentations on the training images to ensure that the transformations are applied correctly.\n",
        "\n",
        "### Key Points:\n",
        "\n",
        "- **Original vs Augmented Images:**  \n",
        "  Each sample is displayed in two rows:\n",
        "  - Top row: original image\n",
        "  - Bottom row: augmented image\n",
        "\n",
        "- **Augmentations Visualized Include:**  \n",
        "  - Random resized crop, color jitter, horizontal flip, rotation  \n",
        "  - Lighting and texture variations  \n",
        "  - Gaussian blur, noise, sharpness adjustments\n",
        "\n",
        "- **Purpose:**  \n",
        "  - Verify that augmentation preserves the class label\n",
        "  - Ensure diversity in training data to improve texture robustness under varying lighting and industrial conditions\n",
        "\n",
        "- **Implementation:**  \n",
        "  - Randomly select a few samples from the training dataset\n",
        "  - Display side-by-side comparisons using Matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "PVUxugeZzmOD",
        "outputId": "0077dd4b-7bc5-457c-b3aa-9dd715d1ed97"
      },
      "outputs": [],
      "source": [
        "# CELL 9 â€“ Visualize Augmented Samples\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "\n",
        "if 'idx_to_class' not in locals():\n",
        "\n",
        "    classes = ['glass', 'metal', 'mixed', 'plastic', 'wood']\n",
        "\n",
        "    idx_to_class = {i: c for i, c in enumerate(classes)}\n",
        "\n",
        "\n",
        "\n",
        "if 'idx_to_class' not in locals():\n",
        "\n",
        "    classes = ['glass', 'metal', 'mixed', 'plastic', 'wood']\n",
        "\n",
        "    idx_to_class = {i: c for i, c in enumerate(classes)}\n",
        "\n",
        "\n",
        "\n",
        "def visualize_augmentations(dataset, num_samples=5):\n",
        "\n",
        "    print(f\"\\nVisualizing {num_samples} augmented samples...\")\n",
        "\n",
        "\n",
        "\n",
        "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
        "\n",
        "\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "\n",
        "        img_path = dataset.image_paths[idx]\n",
        "\n",
        "        try:\n",
        "\n",
        "            # Original image\n",
        "\n",
        "            orig_img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            orig_img_resized = orig_img.resize((224, 224))\n",
        "\n",
        "            axes[0, i].imshow(orig_img_resized)\n",
        "\n",
        "            axes[0, i].set_title(f\"Original\\n{os.path.basename(img_path)[:10]}...\")\n",
        "\n",
        "            axes[0, i].axis('off')\n",
        "\n",
        "\n",
        "\n",
        "            # Augmented image\n",
        "\n",
        "            aug_img, label = dataset[idx]\n",
        "\n",
        "\n",
        "\n",
        "            if isinstance(aug_img, torch.Tensor):\n",
        "\n",
        "                if aug_img.shape[0] == 3:\n",
        "\n",
        "                    img_display = aug_img.clone()\n",
        "\n",
        "                    img_display = img_display.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "                    img_display = np.clip(\n",
        "\n",
        "                        img_display * np.array([0.229, 0.224, 0.225]) +\n",
        "\n",
        "                        np.array([0.485, 0.456, 0.406]),\n",
        "\n",
        "                        0, 1\n",
        "\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "\n",
        "                    img_display = np.zeros((224, 224, 3))\n",
        "\n",
        "            else:\n",
        "\n",
        "                img_display = aug_img\n",
        "\n",
        "\n",
        "\n",
        "            axes[1, i].imshow(img_display)\n",
        "\n",
        "            axes[1, i].set_title(f\"Augmented\\nLabel: {idx_to_class[label]}\")\n",
        "\n",
        "            axes[1, i].axis('off')\n",
        "\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            print(f\"  Error with sample {idx}: {e}\")\n",
        "\n",
        "            axes[0, i].text(0.5, 0.5, \"Error\", ha='center', va='center')\n",
        "\n",
        "            axes[1, i].text(0.5, 0.5, \"Error\", ha='center', va='center')\n",
        "\n",
        "            axes[0, i].axis('off')\n",
        "\n",
        "            axes[1, i].axis('off')\n",
        "\n",
        "\n",
        "\n",
        "    plt.suptitle(\"Texture Robustness under Varying Lighting\\nOriginal vs Augmented Images\",\n",
        "\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(\"âœ“ Visualization completed!\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"Lighting & Texture Augmentation\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "\n",
        "if 'train_aug_dataset' in locals() and len(train_aug_dataset) > 0:\n",
        "\n",
        "    visualize_augmentations(train_aug_dataset, 5)\n",
        "\n",
        "else:\n",
        "\n",
        "    print(\"Creating emergency dataset...\")\n",
        "\n",
        "\n",
        "\n",
        "    class MaterialDataset:\n",
        "\n",
        "        def __init__(self, image_paths, labels, transform=None):\n",
        "\n",
        "            self.image_paths = image_paths\n",
        "\n",
        "            self.labels = labels\n",
        "\n",
        "            self.transform = transform\n",
        "\n",
        "        def __len__(self): return len(self.image_paths)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "\n",
        "            img = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "\n",
        "            if self.transform: img = self.transform(img)\n",
        "\n",
        "            return img, self.labels[idx]\n",
        "\n",
        "\n",
        "\n",
        "    from torchvision import transforms\n",
        "\n",
        "    train_transform_advanced = transforms.Compose([\n",
        "\n",
        "        transforms.RandomResizedCrop(224),\n",
        "\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "    train_aug_dataset = MaterialDataset(X_train[:50], y_train[:50], transform=train_transform_advanced)\n",
        "\n",
        "    visualize_augmentations(train_aug_dataset, min(5, len(train_aug_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDUx44-KepFF"
      },
      "source": [
        "## Final Check: Class Distribution & DataLoader Test\n",
        "\n",
        "In this step, we perform a final verification to ensure that:\n",
        "\n",
        "1. **Class Distribution**  \n",
        "   - Count the number of samples per class in the training, validation, and test splits.  \n",
        "   - Verify that the data is balanced and correctly split.\n",
        "\n",
        "2. **DataLoader Functionality**  \n",
        "   - Load a single batch from `train_aug_loader` to check shapes and values.  \n",
        "   - Print the batch size, image tensor shape, labels, and sample class names.  \n",
        "   - Confirm that image normalization and augmentations are correctly applied.\n",
        "\n",
        "This final check ensures the dataset is ready and the DataLoaders are working properly before training the CNN model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EQUI0p5zxrn",
        "outputId": "8331cc8d-cf27-4ba0-b388-6864fde1093c"
      },
      "outputs": [],
      "source": [
        "# CELL 10 â€“ Final Check: Class Distribution & DataLoader Test\n",
        "\n",
        "import collections\n",
        "\n",
        "\n",
        "\n",
        "if 'idx_to_class' not in locals():\n",
        "\n",
        "    classes = ['glass', 'metal', 'mixed', 'plastic', 'wood']\n",
        "\n",
        "    idx_to_class = {i: c for i, c in enumerate(classes)}\n",
        "\n",
        "\n",
        "\n",
        "if 'idx_to_class' not in locals():\n",
        "\n",
        "    classes = ['glass', 'metal', 'mixed', 'plastic', 'wood']\n",
        "\n",
        "    idx_to_class = {i: c for i, c in enumerate(classes)}\n",
        "\n",
        "\n",
        "\n",
        "# 1. Check class distribution in splits\n",
        "\n",
        "def check_class_distribution(labels, split_name):\n",
        "\n",
        "    print(f\"\\n{split_name} Class Distribution:\")\n",
        "\n",
        "    label_counts = collections.Counter(labels)\n",
        "\n",
        "    for class_idx, count in label_counts.items():\n",
        "\n",
        "        class_name = idx_to_class[class_idx]\n",
        "\n",
        "        print(f\"  {class_name}: {count} samples\")\n",
        "\n",
        "    print(f\"  Total: {len(labels)} samples\")\n",
        "\n",
        "\n",
        "\n",
        "check_class_distribution(y_train, \"TRAIN\")\n",
        "\n",
        "check_class_distribution(y_val, \"VALIDATION\")\n",
        "\n",
        "check_class_distribution(y_test, \"TEST\")\n",
        "\n",
        "\n",
        "\n",
        "# 2. Test DataLoader\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"Testing DataLoader...\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_aug_loader):\n",
        "\n",
        "        print(f\"Batch {batch_idx+1}:\")\n",
        "\n",
        "        print(f\"  Images shape: {images.shape}\")\n",
        "\n",
        "        print(f\"  Labels shape: {labels.shape}\")\n",
        "\n",
        "        print(f\"  Labels: {labels[:5].tolist()} ({[idx_to_class[l.item()] for l in labels[:5]]})\")\n",
        "\n",
        "        print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
        "\n",
        "        break\n",
        "\n",
        "    print(\"\\nâœ… DataLoader is working correctly!\")\n",
        "\n",
        "except Exception as e:\n",
        "\n",
        "    print(f\"\\nâŒ Error in DataLoader: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErExyU6Fz9_C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
